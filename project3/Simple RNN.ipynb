{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Implementing a Simple Recurrent Neural Network (RNN)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, you will design, implement, and evaluate a simple Recurrent Neural Network (RNN) from scratch. This will involve building the entire pipeline, from data preprocessing to model training and evaluation.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Set up TensorFlow or PyTorch environments. You are free to choose your preferred DL platform.\n",
    "2. Use GPU for training.\n",
    "3. Create a data loader and implement data preprocessing where needed.\n",
    "4. Design a Convolutional Neural Network.\n",
    "5. Train and evaluate your model. Make sure to clearly show loss and accuracy values. Include visualizations too.\n",
    "6. Answer assessment questions.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You are free to choose any dataset for this project! Kaggle would be a good source to look for datasets. Below are some examples:\n",
    "- Daily Minimum Temperatures in Melbourne: This dataset contains the daily minimum temperatures in Melbourne, Australia, from 1981 to 1990.\n",
    "- Daily Bitcoin Prices: This dataset contains historical daily prices of Bitcoin, which can be used for time series forecasting projects.\n",
    "- Text8 Dataset: This dataset consists of the first 100 million characters from Wikipedia. It's great for text generation or language modeling tasks.\n",
    "- IMDB Movie Reviews: This dataset contains 50,000 movie reviews for sentiment analysis, split evenly into 25,000 training and 25,000 test sets.\n",
    "- Jena Climate Dataset: This dataset records various weather attributes (temperature, pressure, humidity, etc.) every 10 minutes, making it ideal for time series analysis.\n",
    "- Earthquake Aftershocks: This dataset contains seismic data, suitable for predicting aftershocks following major earthquakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (49580, 280)\n",
      "Labels shape: (49580,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "# Remove HTML tags and URLs from the reviews\n",
    "data['review'] = data['review'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "data['review'] = data['review'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Convert text to lowercase\n",
    "data['review'] = data['review'].str.lower()\n",
    "\n",
    "# Remove special characters\n",
    "data['review'] = data['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Define a more practical maximum sequence length if needed\n",
    "max_len = 280  # You can use this instead of np.max(data['review_length'])\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Prepare labels\n",
    "labels = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "print(\"Data shape:\", padded_sequences.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(10000, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = lstm_out[:, -1]  # get the last time step\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # As used in the tokenizer\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert arrays to tensors\n",
    "train_data = torch.from_numpy(padded_sequences).type(torch.LongTensor)\n",
    "train_labels = torch.from_numpy(labels).type(torch.FloatTensor)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6428901310889952\n",
      "Epoch 2/10, Loss: 0.47934946642768\n",
      "Epoch 3/10, Loss: 0.3117935614912741\n",
      "Epoch 4/10, Loss: 0.2769839208452932\n",
      "Epoch 5/10, Loss: 0.2643570976295779\n",
      "Epoch 6/10, Loss: 0.21631919082614684\n",
      "Epoch 7/10, Loss: 0.19366211922418686\n",
      "Epoch 8/10, Loss: 0.17124731135945168\n",
      "Epoch 9/10, Loss: 0.1536765522269472\n",
      "Epoch 10/10, Loss: 0.13502622269574674\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0fcccbd1d038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming you saved loss values in a list during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_losses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Questions\n",
    "Answer the following questions in detail.\n",
    "\n",
    "1. What is a Recurrent Neural Network (RNN)? Describe its key components and how they differ from those in a feedforward neural network.\n",
    "2. Explain the purpose of the recurrent connection in an RNN. How does it enable the network to handle sequential data?\n",
    "3. What are vanishing and exploding gradients, and how do they affect the training of RNNs?\n",
    "4. Describe the Long Short-Term Memory (LSTM) network and its key components. How does it address the issues of vanishing and exploding gradients?\n",
    "5. What is the purpose of the GRU (Gated Recurrent Unit) in RNNs? Compare it with LSTM.\n",
    "6. Explain the role of the hidden state in an RNN. How is it updated during the training process?\n",
    "7. What are some common evaluation metrics used to assess the performance of an RNN on a sequential task, such as language modeling or time series forecasting?\n",
    "8. How does data preprocessing impact the performance of RNNs? Provide examples of preprocessing steps for text and time series data.\n",
    "9. What is sequence-to-sequence learning in the context of RNNs, and what are its applications?\n",
    "10. How can RNNs be used for anomaly detection in time series data? Describe the general approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Submission\n",
    "Submit a link to your completed Jupyter Notebook (e.g., on GitHub (private) or Google Colab) with all the cells executed, and answers to the assessment questions included at the end of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
